# AI Agent 评估体系：让你的智能体真正"听话"

做过 AI Agent 的都知道一个痛点：好不容易让 Agent 跑起来了，但每次表现都不一样。改完一处bug，另一处又坏了，完全陷入"盲人摸象"的状态。

根本原因是什么？缺少有效的评估体系。

今天我来聊聊 Anthropic 在评估 AI Agent 方面的实战经验。这套方法论，能让你的 Agent 开发从"玄学"变成科学。

## 为什么 Agent 评估这么难

Agent 和传统 LLM 最大的区别，就是它是多轮的。

传统 LLM 一问一答，评估相对简单。Agent 不一样，它会调用工具、修改状态、根据中间结果调整策略。这些让 Agent 变强大的能力，同时也让评估变得极其复杂。

举个真实例子。Anthropic 的 Opus 4.5 在做一个机票预订任务时，发现了一个政策漏洞，"作弊"完成了任务。按 eval 的标准它"失败"了，但实际上它给用户找到了更好的解决方案。

这就是问题所在：你评估的究竟是 Agent 守规矩的程度，还是它解决问题的能力？

## 三种评分器，各有所长

评估 Agent 需要三种评分器：代码评分器、模型评分器、人工评分器。

代码评分器最快、最便宜。比如单元测试、静态分析、精确匹配，这些都是代码评分器的强项。但它有个致命问题：太脆弱了，稍微一点变化就会判定失败。

模型评分器用 LLM 来打分，灵活多了。它可以处理开放性问题，能理解上下文，但也更贵，而且需要人工校准。

人工评分器是黄金标准，但贵得离谱，而且速度慢。

最佳实践是什么？结合使用。代码评分器做基础检查，模型评分器处理复杂判断，人工评分器做周期性校准。

## 不同 Agent 的评估策略

不同类型的 Agent，评估策略完全不同。

编程 Agent 最简单：代码跑得通吗？测试通过吗？SWE-bench Verified 就是这么干的，给 Agent 真实的 GitHub issue，通过运行测试套件来评分。

对话 Agent 的挑战更大。你不仅要看任务有没有完成，还要看交互质量怎么样。工单解决了吗？10 轮内完成了吗？语气得体吗？这些都是要考量的。

研究 Agent 更难评估。什么是"全面"？什么是"有依据"？这完全取决于上下文。市场调研、收购尽调、科学报告，各自需要完全不同的标准。

## 处理非确定性的两个指标

Agent 每次运行结果都不一样，怎么评估？

两个关键指标：pass@k 和 pass^k。

pass@k 是 k 次尝试中至少成功一次的概率。适合"一次成功就够了"的场景，比如代码生成。

pass^k 是 k 次试验全部成功的概率。适合"每次都需要成功"的场景，比如客服 Agent。

这两个指标随着 k 增大会呈现相反的趋势：pass@k 接近 100%，pass^k 接近 0%。

## 从零构建评估体系

不要等到有几百个测试用例才开始。20 到 50 个来自真实失败案例的简单任务，就是很好的起点。

从手动测试开始，把开发过程中手动检查的项目、用户报告的失败，转化成测试用例。

编写明确的任务和参考解。好的任务，两个领域专家会得出相同的通过/失败结论。

构建平衡的问题集。既要测试应该发生的行为，也要测试不应该发生的行为。

最重要的，是检查记录。阅读记录是验证评估是否测到关键东西的关键技能。失败应该是公平的，Agent 哪里错了、为什么错，应该很清楚。

## 评估与其他方法的关系

自动评估不是唯一的评估方式。生产监控、A/B 测试、用户反馈、人工记录审查，都是重要的评估手段。

最有效的团队结合多种方法：自动评估用于快速迭代，生产监控用于验证，定期人工审查用于校准。

这就像瑞士奶酪模型，没有任何单一层能捕捉所有问题。多层叠加，漏洞才能被堵住。

## 写在最后

没有 evals 的团队，会陷入被动循环。修复一个失败，创造另一个，完全不知道是不是真正的回退。

早期投资的团队会发现，发展反而更快。失败变成测试用例，测试用例防止回退，指标取代猜测。

但价值只有在你将 evals 视为核心组件而非事后诸葛亮时，才会累积。

AI Agent 评估还是个新领域，但这个方向值得投入。越早开始，收益越大。
